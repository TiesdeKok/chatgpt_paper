{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0677690b-bc59-41b1-bf6c-1340b5f6e4a8",
   "metadata": {},
   "source": [
    "# OpenAI - zero shot example\n",
    "\n",
    "The guide is a companion to the paper *\"Generative LLMs and Textual Analysis in Accounting:(Chat)GPT as Research Assistant?\"* ([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4429658))\n",
    "\n",
    "**Author:** [Ties de Kok](https://www.tiesdekok.com)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fdde1-c830-4edd-a062-56592955a0cf",
   "metadata": {},
   "source": [
    "----\n",
    "# Imports\n",
    "----\n",
    "\n",
    "\n",
    "All the dependencies required for this notebook are provided in the `environment.yml` file.\n",
    "\n",
    "To install: `conda env create -f environment.yml` --> this creates the `gllm` environment.\n",
    "\n",
    "I recommend using Python 3.9 or higher to avoid dependency conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1969ecc-dd72-475a-ae51-79bdc83c84d7",
   "metadata": {},
   "source": [
    "**Python built-in libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ec5462-905d-4b7c-b748-a43dbbe84b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, copy, random, json, time, datetime\n",
    "from pathlib import Path\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae5855-5a12-4098-9545-495fbc540506",
   "metadata": {},
   "source": [
    "**Libraries for interacting with the OpenAI API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77af21a-0a1c-4610-a213-a4565742bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebae9d-fd4c-42da-81ea-7b4672bbec07",
   "metadata": {},
   "source": [
    "**General helper libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c877637b-27c2-4d17-934d-a1e0c65422fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95257da-1df7-44de-8cc9-49071d6db842",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ccdeb1-e3f2-4f96-b924-9a36287e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b98b8-afcd-40b6-a2cb-173f95a14e98",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da331675-1d54-41a2-9c04-c77d331c3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function makes it easier to print rendered markdown through a code cell.\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def mprint(text, *args, **kwargs):\n",
    "    if 'end' in kwargs.keys():\n",
    "        text += kwargs['end']\n",
    "        \n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52592dcd-1e6e-4740-b6bd-aa8765d58c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "# Toy example\n",
    "----\n",
    "\n",
    "I will use a hypothetical dataset of earnings call sentences and try to identify sentences with a forward-looking statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c2b8c-b553-44f6-9443-344c1a508941",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa46d21-8c65-4af1-9381-fd4025af1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.cwd() / \"data\" / \"statements.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    statement_list = json.load(f)\n",
    "\n",
    "statement_df = pd.DataFrame(statement_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e367301a-2e1f-41cb-95cb-6af74f95db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = statement_df.iloc[0].statement\n",
    "print(sentence_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562b95b-96fc-4f15-91e5-5768a4d0c6f6",
   "metadata": {},
   "source": [
    "----\n",
    "## Prompt engineering\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64655d5-f2f1-4af7-a252-7f5ea05fa1b1",
   "metadata": {},
   "source": [
    "### Define prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c4c8a7-3155-4c3d-ac7f-9cd4f8148891",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Task: classify whether the statement below contains a forward looking statements (fls).\n",
    "Rules:\n",
    "- Answer using JSON in the following format: {{\"contains_fls\" : 0 or 1}}\n",
    "Statement:\n",
    "> {statement}\n",
    "JSON =\n",
    "\"\"\".strip()\n",
    "\n",
    "## Note, the curly braces are what we will fill in for each observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19a4ee-b97b-446e-942e-f13d27334e5e",
   "metadata": {},
   "source": [
    "### Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cbe27a0-b166-47b7-a7e3-350f3a7bbc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(**{\n",
    "    \"statement\" : sentence_1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36ae831-9f69-4c4a-8733-abf2c5cb4b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classify whether the statement below contains a forward looking statements (fls).\n",
      "Rules:\n",
      "- Answer using JSON in the following format: {\"contains_fls\" : 0 or 1}\n",
      "Statement:\n",
      "> In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.\n",
      "JSON =\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f65fe-b307-4460-96cd-3ff300233c20",
   "metadata": {},
   "source": [
    "### How many tokens will this use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "409ddcd8-8abc-486a-9087-e471d25eef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4810fd6d-349c-49c2-a6b2-cfdc225b0817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt:**\n",
       "```\n",
       "Task: classify whether the statement below contains a forward looking statements (fls).\n",
       "Rules:\n",
       "- Answer using JSON in the following format: {\"contains_fls\" : 0 or 1}\n",
       "Statement:\n",
       "> In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.\n",
       "JSON =\n",
       "```\n",
       "<br>\n",
       "\n",
       "**Number of tokens:**\n",
       "`70`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(f\"\"\"\n",
    "**Prompt:**\n",
    "```\n",
    "{prompt}\n",
    "```\n",
    "<br>\n",
    "\n",
    "**Number of tokens:**\n",
    "`{len(encoder.encode(prompt)):,}`\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6682f-faa4-4b4c-9b39-bcb383aede60",
   "metadata": {},
   "source": [
    "---\n",
    "## Set up OpenAI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853aba4-baea-4589-b18c-e64427f5c0f1",
   "metadata": {},
   "source": [
    "There are roughly three ways to interact with the OpenAI API through Python:\n",
    "\n",
    "- Directly using `requests`   \n",
    "- Using the official `openai` Python library   \n",
    "- Through a higher level library such as `langchain`\n",
    "\n",
    "To use the OpenAI API you will need an API key. If you don't have one, follow these steps:   \n",
    "\n",
    "1. Create an OpenAI account --> https://platform.openai.com   \n",
    "2. Create an OpenAI API key --> https://platform.openai.com/account/api-keys   \n",
    "3. You will get \\\\$5 in free credits if you create a new account. If you've used that up, you will need to add a payment method. The code in this notebook will cost less than a dollar to run.\n",
    "\n",
    "Once you have your OpenAI Key, you can set it as the `OPENAI_API_KEY` environment variable (recommended) or enter it directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b60c5fef-cc7c-4eaf-9e94-5ddaf999a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass(prompt='Enter your API key: ')\n",
    "    \n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "    \n",
    "## KEEP YOUR KEY SECURE, ANYONE WITH ACCESS TO IT CAN GENERATE COSTS ON YOUR ACCOUNT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02130891-f719-4954-a843-e99d057c40ae",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80646895-fca6-4953-ad30-75de12c95367",
   "metadata": {},
   "source": [
    "##### Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3121243a-dc99-4aba-ac61-1d7a57b2ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0 ## Setting this to 0 makes the generations, mostly, deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6cd69-114c-416c-bbba-0079f062c8ff",
   "metadata": {},
   "source": [
    "##### Set the system message\n",
    "\n",
    "The chat models, `gpt-3.5-turbo` and `gpt-4` also accept a so-called system message. These models are specifically trained to follow the role that is explained to them in this system message. The `gpt-3.5-turbo` message does not always pay strong attention to this system message, however, GPT-4 does. \n",
    "\n",
    "The default system message is \"You are a helpful assistant.\"\n",
    "\n",
    "For more details, see: https://platform.openai.com/docs/guides/chat/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19c12b8a-410a-4b61-8b04-e77764877403",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a serious research assistant who follows exact instructions and returns only valid JSON.\"\n",
    "## Note, the system message also counts towards our token usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0d0b8-25b5-47e3-a394-0dee9a015202",
   "metadata": {},
   "source": [
    "### Make generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6964de99-aebc-4e6e-a726-f0ee260da815",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model = model,\n",
    "    temperature = temperature,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\" : system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6215e-7143-47c0-b5d5-daf1dee971cb",
   "metadata": {},
   "source": [
    "### The result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c85fdd20-a110-4962-85e7-1cdc9d8153c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef498e54-86ce-40cf-8329-13ce796ecba5",
   "metadata": {},
   "source": [
    "**How many tokens did we just use?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6335e205-75b3-47a5-8b95-29aa5be8fd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 97, 'completion_tokens': 9, 'total_tokens': 106}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(completion.usage) ## Higher than our estimate because of chat token overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8f905-fa85-4e23-a136-e84de8df4496",
   "metadata": {},
   "source": [
    "**Did it work?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cf5d513-f325-4541-860b-35503eb1686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': '{\"contains_fls\" : 0}'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = dict(completion[\"choices\"][0][\"message\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1116c98f-281d-4a71-b310-0363a8d25a8b",
   "metadata": {},
   "source": [
    "---\n",
    "# Function demonstration\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f908a-2205-4c5c-99cc-4a05067fe2cf",
   "metadata": {},
   "source": [
    "## Let's wrap that up into some functions\n",
    "\n",
    "We can wrap the above logic up into a function so that we can scale thing more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "337ab99f-1aad-4648-b201-0f27459f2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data, prompt_template):\n",
    "    prompt = prompt_template.format(**{\n",
    "        \"statement\" : data[\"statement\"].strip()\n",
    "    })\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2550c504-cc75-4a64-9732-e1a6d48e69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(\n",
    "    i, ## Adding an identifier will make things easier to track and match up later.\n",
    "    prompt, \n",
    "    model = model,\n",
    "    temperature = temperature,\n",
    "    system_message = system_message\n",
    "    ):\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        temperature = temperature,\n",
    "        stop = [\"}\"], ## This forces the model to stop when it hits a closing brace\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\" : system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    result = dict(completion[\"choices\"][0][\"message\"])\n",
    "    prediction = json.loads(result[\"content\"] + \"}\") \n",
    "    ## The end is not included, so we add it back, hence the + \"}\"\n",
    "    prediction[\"i\"] = i\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65e42e-4519-4048-b59e-d1ee7ad96345",
   "metadata": {},
   "source": [
    "### Loop through first five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa58c74-a349-4669-a1ba-4d3ddcf1ccd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Item:** `1`\n",
       "> In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.    \n",
       "\n",
       "*Prediction - contains FLS:* `0`\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Item:** `2`\n",
       "> We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.    \n",
       "\n",
       "*Prediction - contains FLS:* `1`\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Item:** `3`\n",
       "> Our recent acquisition of XYZ Company has already started to show positive results in terms of cost savings and market reach.    \n",
       "\n",
       "*Prediction - contains FLS:* `0`\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Item:** `4`\n",
       "> We expect to see continued growth in the Asian market, with a potential increase in revenue of 25% over the next three years.    \n",
       "\n",
       "*Prediction - contains FLS:* `1`\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Item:** `5`\n",
       "> In the past year, we have successfully reduced our operational costs by 10% through process improvements and better supply chain management.    \n",
       "\n",
       "*Prediction - contains FLS:* `0`\n",
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, row in statement_df.head().iterrows():\n",
    "    prompt = generate_prompt(row, prompt_template)\n",
    "    prediction = make_prediction(row[\"i\"], prompt) \n",
    "    mprint(f\"\"\"\n",
    "**Item:** `{row[\"i\"]}`\n",
    "> {row[\"statement\"]}    \n",
    "\n",
    "*Prediction - contains FLS:* `{prediction[\"contains_fls\"]}`\n",
    "<br><br>\n",
    "\"\"\".strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
