{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c239bf9-4ddc-448a-966a-6f2155eedb3f",
   "metadata": {},
   "source": [
    "# Local fine-tuning example - Mistral 7b & Microsoft Phi-2 2.7b\n",
    "This notebook is a companion to the following paper, please cite it accordingly: \n",
    "- [de Kok (2024) - SSRN](https://papers.ssrn.com/abstract=4429658)\n",
    "\n",
    "\n",
    "**Author:** [Ties de Kok](https://www.tiesdekok.com/)\n",
    "\n",
    "**Important:** please read the instructions in the corresponding `readme.md` file for the setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d69567-890c-4d6c-903b-cb707c183661",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d43702e-172f-429a-8ea0-432ba6b408f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, random, copy, json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec881e-47b2-4366-84d6-608b394a7bbd",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb44932-bb46-48f5-ab4f-2f61e7cf0b67",
   "metadata": {},
   "source": [
    "##### Download the training data\n",
    "\n",
    "You can upload your own training data to the Runpod intance using either the Jupyter Lab upload button, SFTP, or the Cloud Sync option in the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d8c2aa-477c-4fb4-8640-358e4ddf2533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-27 15:50:41--  https://raw.githubusercontent.com/TiesdeKok/chatgpt_paper/main/code_examples/vast_ai/data/training_data.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11265 (11K) [text/plain]\n",
      "Saving to: ‘training_data.jsonl’\n",
      "\n",
      "training_data.jsonl 100%[===================>]  11.00K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-01-27 15:50:41 (63.9 MB/s) - ‘training_data.jsonl’ saved [11265/11265]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/TiesdeKok/chatgpt_paper/main/code_examples/vast_ai/data/training_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31e0dd-d24c-4efe-acdd-c8cc1bf161a9",
   "metadata": {},
   "source": [
    "##### Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2176c5-9f94-4326-add0-1fb7bce9370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "with open(\"training_data.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        training_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b828bb-82e0-4b8e-a1d7-873341683996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INSTRUCTION': 'Text:\\nWe expect the implementation of our new data-driven pricing strategy <to> result in a 5% <increase> in gross margin over the next two quarters.\\n####\\n',\n",
       " 'RESPONSE': '[\"to\", \"increase\"] <|end|>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271336f4-00fe-4aae-b45f-d2b92302f7e6",
   "metadata": {},
   "source": [
    "##### Prepare the data for LLaMa-Factory\n",
    "*Notes:* \n",
    "- Technically this isn't nescessary here as we the jsonl is already in a format that is loadable. However, I am demonstrating the below so that you can adapt it to your own data.\n",
    "- I am removing the special tokens at the beginning and end as `LlaMA-Factory` will add them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00121a90-22c0-4773-b25a-b5b7dec6f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "for item in training_data:\n",
    "    items.append({\n",
    "        \"prompt\" : item[\"INSTRUCTION\"][6:].replace(\"\\n####\\n\", \"\"), ## This way we remove the special tokens at the start and end, which llamafactory will add by itself\n",
    "        \"completion\" : item[\"RESPONSE\"].replace(\" <|end|>\", \"\") ## Same, remove EOS token\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d3a3fe-e921-4824-a331-bc138fdbb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(items, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcd51d-6e05-4834-a041-f01a260edf68",
   "metadata": {},
   "source": [
    "##### Add our dataset to the datasets of LlaMA-Factory\n",
    "\n",
    "LlaMA-Factory requires that the dataset is included in the `dataset_info.json` file.  \n",
    "The dataset itself can be located anywhere on your drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bdfd3b-6573-4898-b886-decba0ba72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/workspace/LLaMA-Factory/data/dataset_info.json\", \"r\") as f:\n",
    "    dataset_info = json.loads(f.read())\n",
    "\n",
    "dataset_info[\"my_data\"] = {\n",
    "    \"file_name\" : \"/workspace/data.json\",\n",
    "    \"columns\" : {\n",
    "        \"prompt\" : \"prompt\",\n",
    "        \"response\" : \"completion\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"/workspace/LLaMA-Factory/data/dataset_info.json\", \"w\") as f:\n",
    "    json.dump(dataset_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd151e8-828d-4e1f-8ac3-dd1e5e24582d",
   "metadata": {},
   "source": [
    "-----\n",
    "# Mistral-7B-Instruct\n",
    "----\n",
    "\n",
    "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38e83a-1dc1-4cc6-b825-3198500fa5c6",
   "metadata": {},
   "source": [
    "## For a full fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b83655-4b2e-42c7-bcb3-cd0a0e003d20",
   "metadata": {},
   "source": [
    "##### For Deepspeed to work we need a config\n",
    "\n",
    "This is an adapted version from the default from the LlaMA-Factory Github page.\n",
    "\n",
    "Note: the \"offload_optimizer\" --> CPU step is crucial here. Without it you will get a CUDA OOM error. \n",
    "The downside is that training is slower. \n",
    "\n",
    "For more details on the Deepspeed config setting, see e.g.:   \n",
    "https://huggingface.co/docs/accelerate/v0.11.0/en/deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c46bda7a-057d-4f37-b2c9-1bf96ed8e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json ## Saves having to scroll back up after kernel restart. :)\n",
    "\n",
    "ds_config = {\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"zero_allow_untested_optimizer\": True,\n",
    "  \"fp16\": {\n",
    "    \"enabled\": \"auto\",\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"allgather_partitions\": True,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"reduce_scatter\": False,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": False,\n",
    "    \"contiguous_gradients\": True,\n",
    "    \"offload_optimizer\": {\n",
    "        \"device\": \"cpu\",\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"/workspace/ds_config_full.json\", \"w\") as f:\n",
    "    json.dump(ds_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffdc3c-e45b-4767-ba9d-e753998b21bf",
   "metadata": {},
   "source": [
    "### Our command\n",
    "\n",
    "Copy the below into the terminal and keep your fingers crossed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da874641-6700-41da-b489-17b02353e608",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd /workspace/LLaMA-Factory\n",
    "deepspeed --num_gpus 5 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed /workspace/ds_config_full.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path mistralai/Mistral-7B-Instruct-v0.1 \\\n",
    "    --dataset \"my_data\" \\\n",
    "    --template default \\\n",
    "    --finetuning_type full \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir \"/workspace/ft_full_1\" \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 3 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 30.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dff1cc-03ad-4244-b305-f4c35f10e8b5",
   "metadata": {},
   "source": [
    "## For a partial fine-tune (i.e., LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef5f43-4c7d-44c1-aeaf-ff2c5e4ee714",
   "metadata": {},
   "source": [
    "##### For Deepspeed to work we need a config, this is the default from the LlaMA-Factory Github page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e3caf4-48d1-45c7-bde7-3b1ea1c37ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json ## Duplicative, but saves having to scroll back up after kernel restart. :)\n",
    "\n",
    "ds_config = {\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"zero_allow_untested_optimizer\": True,\n",
    "  \"fp16\": {\n",
    "    \"enabled\": \"auto\",\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 1,\n",
    "    \"allgather_partitions\": True,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"reduce_scatter\": False,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": False,\n",
    "    \"contiguous_gradients\": True,\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"/workspace/ds_config_lora.json\", \"w\") as f:\n",
    "    json.dump(ds_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acb56c-404c-44eb-b11f-05b4987c7299",
   "metadata": {},
   "source": [
    "### Our command\n",
    "\n",
    "Copy the below into the terminal and keep your fingers crossed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c867b2-ce28-4704-a420-15b7ff603575",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd /workspace/LLaMA-Factory\n",
    "deepspeed --num_gpus 5 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed /workspace/ds_config_lora.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path mistralai/Mistral-7B-Instruct-v0.1 \\\n",
    "    --dataset \"my_data\" \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora  \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir \"/workspace/ft_lora_1\" \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 3 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 100.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bce7b-8461-411a-aa7e-3e32800ce1b7",
   "metadata": {},
   "source": [
    "-----\n",
    "# microsoft/phi-2 (2.7b)\n",
    "----\n",
    "https://huggingface.co/microsoft/phi-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9afef1d-621b-4550-aedb-91cfecf9b6a7",
   "metadata": {},
   "source": [
    "## For a full fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9519b7c-2696-452d-b68c-e681052678d6",
   "metadata": {},
   "source": [
    "##### For Deepspeed to work we need a config\n",
    "\n",
    "This is an adapted version from the default from the LlaMA-Factory Github page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6175fc-1721-45b4-88da-15d85a7a9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json ## Duplicative, but saves having to scroll back up after kernel restart. :)\n",
    "\n",
    "ds_config = {\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"zero_allow_untested_optimizer\": True,\n",
    "  \"fp16\": {\n",
    "    \"enabled\": \"auto\",\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"allgather_partitions\": True,\n",
    "    \"allgather_bucket_size\": 5e8,\n",
    "    \"reduce_scatter\": False,\n",
    "    \"reduce_bucket_size\": 5e8,\n",
    "    \"overlap_comm\": False,\n",
    "    \"contiguous_gradients\": True,\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"/workspace/ds_config_full_phi2.json\", \"w\") as f:\n",
    "    json.dump(ds_config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03804276-b404-452c-8286-433c3132f4f6",
   "metadata": {},
   "source": [
    "### Our command\n",
    "\n",
    "Copy the below into the terminal and keep your fingers crossed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ea20b-8519-49f6-b5d7-294521d29889",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd /workspace/LLaMA-Factory\n",
    "deepspeed --num_gpus 5 --master_port=9901 src/train_bash.py \\\n",
    "    --deepspeed /workspace/ds_config_full_phi2.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path microsoft/phi-2 \\\n",
    "    --dataset \"my_data\" \\\n",
    "    --template default \\\n",
    "    --finetuning_type full \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir \"/workspace/ft_full_phi2_1\" \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 40.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ee0dc-de30-44f0-b7a8-75d639746ae4",
   "metadata": {},
   "source": [
    "----\n",
    "# Use the resulting model\n",
    "---\n",
    "\n",
    "**WARNING:** After running the below you need to restart the kernel before starting your next fine-tune!\n",
    "\n",
    "An easy way to download your model to your computer is to enter the model folder on the left and right click --> \"Download Current Folder as Archive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d08917-62e4-4840-af20-6a1ada81c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5e01cdc-4364-43d9-96ea-e125f4f4424c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9884c2a46f214d4b8b079078935b5967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#model = \"ft_full_1\"\n",
    "#model = \"ft_lora_1\"\n",
    "model = \"ft_full_phi2_1\"\n",
    "\n",
    "model_path = Path(f\"/workspace/{model}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2300a827-82b0-4a5c-b7f9-212ceb6200ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: In the year 2021 our <production> <numbers> are looking a little down, by about 3%.\n",
      "Assistant:[\"production\", \"numbers\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['production', 'numbers']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In the year 2021 our <production> <numbers> are looking a little down, by about 3%.\" \n",
    "inputs = tokenizer(\n",
    "    f\"\"\"Human: {text}\\nAssistant:\"\"\",\n",
    "    return_tensors = \"pt\",\n",
    "    return_attention_mask = \"False\",    \n",
    ")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length = 50,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
    "print(text)\n",
    "\n",
    "result = json.loads(text.split(\"\\nAssistant:\")[-1])\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
