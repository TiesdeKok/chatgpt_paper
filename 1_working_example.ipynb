{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0677690b-bc59-41b1-bf6c-1340b5f6e4a8",
   "metadata": {},
   "source": [
    "# Technical guide #1: GLLM working example\n",
    "\n",
    "This technical guide shows how to implement a textual analysis task using a GLLM model. \n",
    "\n",
    "The guide is a companion to the paper *\"Generative LLMs and Textual Analysis in Accounting:(Chat)GPT as Research Assistant?\"* ([SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4429658))\n",
    "\n",
    "**Author:** Ties de Kok    \n",
    "**Last updated:** April 2023    \n",
    "**Status:** *Early - work-in-progress*    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fdde1-c830-4edd-a062-56592955a0cf",
   "metadata": {},
   "source": [
    "----\n",
    "# Imports\n",
    "----\n",
    "\n",
    "\n",
    "All the dependencies required for this notebook are provided in the `environment.yml` file.\n",
    "\n",
    "To install: `conda env create -f environment.yml` --> this creates the `gllm` environment.\n",
    "\n",
    "I recommend using Python 3.9 or higher to avoid dependency conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1969ecc-dd72-475a-ae51-79bdc83c84d7",
   "metadata": {},
   "source": [
    "**Python built-in libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4ec5462-905d-4b7c-b748-a43dbbe84b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, copy, random, json, time, datetime\n",
    "from pathlib import Path\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae5855-5a12-4098-9545-495fbc540506",
   "metadata": {},
   "source": [
    "**Libraries for interacting with the OpenAI API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77af21a-0a1c-4610-a213-a4565742bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebae9d-fd4c-42da-81ea-7b4672bbec07",
   "metadata": {},
   "source": [
    "**General helper libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c877637b-27c2-4d17-934d-a1e0c65422fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbc05f0-c9e2-4985-9279-1fab4cce31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95257da-1df7-44de-8cc9-49071d6db842",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ccdeb1-e3f2-4f96-b924-9a36287e78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b98b8-afcd-40b6-a2cb-173f95a14e98",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da331675-1d54-41a2-9c04-c77d331c3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function makes it easier to print rendered markdown through a code cell.\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def mprint(text, *args, **kwargs):\n",
    "    if 'end' in kwargs.keys():\n",
    "        text += kwargs['end']\n",
    "        \n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52592dcd-1e6e-4740-b6bd-aa8765d58c66",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "# Task description\n",
    "----\n",
    "\n",
    "Let's say the objective of our toy problem is as follows: \n",
    "\n",
    "> Identify earnings call sentences that contain a forward-looking statement.\n",
    "\n",
    "In this notebook we will create a prediction pipeline to accomplish this using the OpenAI ChatGPT (*gpt-3.5-turbo*) API. \n",
    "\n",
    "An example of a FLS sentence would be:\n",
    "\n",
    "> We anticipate that sales in our Europe segment will go up. \n",
    "\n",
    "An example of a non-FLS sentence would be:\n",
    "\n",
    "> Our Europe segment is performing well and the sales numbers are up relative to last quarter. \n",
    "\n",
    "For example, a question that tries to elicit a FLS would be: \n",
    "\n",
    "**Disclaimer:**\n",
    "\n",
    "As discussed in the paper, GLLMs are generally best suited to solve problems that would otherwise require a machine learning approach or manual coding. The objective of this toy example is to illustrate the basic coding steps to implement a GLLM pipeline, not to illustrate the best type of problem to solve using a GLLM model. This specific task can likely also be solved using traditional methods such as FLS keyword lists, but it makes for an example that is easier to follow and adapt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c2b8c-b553-44f6-9443-344c1a508941",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa46d21-8c65-4af1-9381-fd4025af1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.cwd() / \"data\" / \"statements.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    statement_list = json.load(f)\n",
    "\n",
    "statement_df = pd.DataFrame(statement_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db449a7d-c268-4994-aa7d-6e4c38e2c4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We have **60** statements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint(f\"We have **{len(statement_list)}** statements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0add88a8-a4cc-4671-96b9-4bed01f052fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>statement</th>\n",
       "      <th>contains_fls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>We are excited about the possibilities that li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>In the past year, we have successfully reduced...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Our ongoing digital transformation efforts are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We expect to see continued growth in the Asian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>The successful rollout of our new customer loy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     i                                          statement  contains_fls\n",
       "56  57  We are excited about the possibilities that li...             1\n",
       "4    5  In the past year, we have successfully reduced...             0\n",
       "17  18  Our ongoing digital transformation efforts are...             1\n",
       "3    4  We expect to see continued growth in the Asian...             1\n",
       "32  33  The successful rollout of our new customer loy...             0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a7c80-d08b-45b7-b617-93738595ce7d",
   "metadata": {},
   "source": [
    "### Select a demo example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87a93f-f9de-43b2-9f48-91bb0b0c9497",
   "metadata": {},
   "source": [
    "#### An example with an FLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c73d390-0eca-4281-a519-383a25535316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'statement': 'We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.',\n",
       " 'contains_fls': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fls_row = statement_df[statement_df.contains_fls == 1].iloc[0].to_dict()\n",
    "fls_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96091d43-2673-4374-8cdd-db97b3d455a2",
   "metadata": {},
   "source": [
    "#### An example without an FLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dcbf1e7-eb27-45b2-b5d6-27552be97d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'statement': 'In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.',\n",
       " 'contains_fls': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_fls_row = statement_df[statement_df.contains_fls == 0].iloc[0].to_dict()\n",
    "non_fls_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3236f-8d94-4e30-be14-0cb898d9c2bb",
   "metadata": {},
   "source": [
    "-------\n",
    "# #1 - Understanding the requirements\n",
    "-----\n",
    "\n",
    "**What information is nescessary to know to complete the task?**\n",
    "\n",
    "On order to evaluate whether a statement is forward-looking or not one likely needs to know the following things:\n",
    "\n",
    "1. The statement (obviously)  \n",
    "2. The definition of what would consistute a forward-looking statement\n",
    "\n",
    "Things yields a few questions / discussion points:\n",
    "\n",
    "- It likely won't matter if we include additional details, such as the company name, the managers name(s), or the analysts' name. We could add these as additional context, however, that will eat into our token-budget and it might distract the model from our primary task. \n",
    "- It is not immediately clear whether it is nescessary to provide the model with an explicit definition, or examples, of what constitutes a forward-looking statement. Doing so would require more tokens, so we can try it without an explicit definition first and adjust things from there. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440a277-9ad1-49a4-80f3-206886f550d1",
   "metadata": {},
   "source": [
    "----\n",
    "# #2 - Decide on approach and model\n",
    "----\n",
    "\n",
    "**What approach to try first?**\n",
    "\n",
    "Based on our evaluation above, it is quite plausible that a larger GLLM model, such as ChatGPT (gpt-3.5-turbo) or GPT-4, could perform this task in a *zero-shot* fashion. I consider this plausible because (1) the amount of context required is small, (2) the task is reasonably easy to do for a human, and (3) ad-hoc experimentation with a few random examples in ChatGPT shows promising results. \n",
    "\n",
    "--> So let's try the *zero-shot* approach first. We can always switch to the *few-shot* or *fine-tuning* approaches later.\n",
    "\n",
    "**What model?**\n",
    "\n",
    "*Zero-shot* approaches work best with larger instruct-tuned models, such as ChatGPT/GPT-4. At the time of writing, the ChatGPT (`gpt-3.5-turbo`) API endpoints provided by OpenAI are both powerful and cost-efficient. The GPT-4 API would likely work better, however, it is significantly slower, more expensive, and by invite-only. \n",
    "\n",
    "The ChatGPT API also does not use the prompts & completions for training purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562b95b-96fc-4f15-91e5-5768a4d0c6f6",
   "metadata": {},
   "source": [
    "----\n",
    "# #3 - Prompt engineering\n",
    "----\n",
    "\n",
    "There are a few things we need to achieve with out prompt template:\n",
    "\n",
    "1. It needs to properly instruct the model to correctly identify FLS.  \n",
    "2. It needs to yield a result that is consistent and easily parseable for us.  \n",
    "3. It needs to use up as few tokens as possible. \n",
    "\n",
    "We can design our prompt in many different ways, for example we could write it like we would to a human:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364a0e4e-8abe-453a-8b74-a6a0ad5eab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_v1 = \"\"\"\n",
    "You are a research assistant and your task is to classify whether the following statement from an earnings conference call possibly making a forward-looking statement (FLS). Return the results in valid JSON format using the format {{\"fls\" : 0 or 1}}. The statement is:\n",
    "{statement}\n",
    "\"\"\".strip()\n",
    "\n",
    "## Note, we need to use double curly braces for the output format so that our \"format\" operation below does expect us to fill it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6870-d482-4d4a-b7d6-f29b4efdcda2",
   "metadata": {},
   "source": [
    "This prompt might work, however, we can improve it in several ways:\n",
    "\n",
    "1. We can reduce the number of tokens.    \n",
    "2. We can remove ambigious language such as \"possibly\".    \n",
    "3. We can make the model more likely to yield the desirable output.   \n",
    "4. We can make it easier to add additional instructions by using a list.    \n",
    "\n",
    "For example by creating a prompt like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c4c8a7-3155-4c3d-ac7f-9cd4f8148891",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_v2 = \"\"\"\n",
    "Task: classify whether the statement below contains a forward looking statements (fls).\n",
    "Rules:\n",
    "- Answer using JSON in the following format: {{\"contains_fls\" : 0 or 1}}\n",
    "Statement:\n",
    "> {statement}\n",
    "JSON =\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c5d04-22b6-4112-94e4-96ceddf16d87",
   "metadata": {},
   "source": [
    "### Let's fill these templates to see what they would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a17769f5-ce4a-41c0-9079-d1a7f796346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\n",
    "    \"statement\" : fls_row[\"statement\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710c7836-a656-4ce8-9b56-5bc55ee93ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a research assistant and your task is to classify whether the following statement from an earnings conference call possibly making a forward-looking statement (FLS). Return the results in valid JSON format using the format {\"fls\" : 0 or 1}. The statement is:\n",
      "We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = prompt_template_v1.format(**input_dict)\n",
    "print(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8fe50c-6a35-43c0-beec-39638c0b3b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classify whether the statement below contains a forward looking statements (fls).\n",
      "Rules:\n",
      "- Answer using JSON in the following format: {\"contains_fls\" : 0 or 1}\n",
      "Statement:\n",
      "> We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.\n",
      "JSON =\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = prompt_template_v2.format(**input_dict)\n",
    "print(prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8cee1-9a8f-43e4-b027-5347c5f4a14f",
   "metadata": {},
   "source": [
    "### How much tokens would each of these use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78942b-fb86-48ba-a726-05f22fb4c0a5",
   "metadata": {},
   "source": [
    "#### Set up `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4abd0ec-78ea-4b93-b1f1-82fcf89a448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2618dab-39dd-4a36-8494-c221b6b32308",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e014116-59ed-470a-97d3-194778d3ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first prompt template requires 79 tokens'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"The first prompt template requires {len(encoder.encode(prompt_1)):,} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4c62ec-860e-4658-98c4-614347e98bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first prompt template requires 68 tokens'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"The first prompt template requires {len(encoder.encode(prompt_2)):,} tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc37499-5335-4c7a-83af-e3f7e23075b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remember, 20% less tokens == 20% lower costs + 20% faster! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5ba8c-9b35-4f98-b6d9-700c08a009bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89645fbe-ba51-420a-85dd-01243cfde30a",
   "metadata": {},
   "source": [
    "### Set up OpenAI\n",
    "\n",
    "There are roughly three ways to interact with the OpenAI API through Python:\n",
    "\n",
    "- Directly using `requests`   \n",
    "- Using the official `openai` Python library   \n",
    "- Through a higher level library such as `langchain`\n",
    "\n",
    "To use the OpenAI API you will need an API key. If you don't have one, follow these steps:   \n",
    "\n",
    "1. Create an OpenAI account --> https://platform.openai.com   \n",
    "2. Create an OpenAI API key --> https://platform.openai.com/account/api-keys   \n",
    "3. You will get \\\\$5 in free credits if you create a new account. If you've used that up, you will need to add a payment method. The code in this notebook will cost less than a dollar to run.\n",
    "\n",
    "Once you have your OpenAI Key, you can set it as the `OPENAI_API_KEY` environment variable (recommended) or enter it directly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ced591e2-16a5-4bc2-a684-7127f0af19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass(prompt='Enter your API key: ')\n",
    "    \n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "    \n",
    "## KEEP YOUR KEY SECURE, ANYONE WITH ACCESS TO IT CAN GENERATE COSTS ON YOUR ACCOUNT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ea9b6-d27d-4fae-8f8e-39be492b4705",
   "metadata": {},
   "source": [
    "### Define API parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80646895-fca6-4953-ad30-75de12c95367",
   "metadata": {},
   "source": [
    "##### Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3121243a-dc99-4aba-ac61-1d7a57b2ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "temperature = 0 ## Setting this to 0 makes the generations, mostly, deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6cd69-114c-416c-bbba-0079f062c8ff",
   "metadata": {},
   "source": [
    "##### Set the system message\n",
    "\n",
    "The chat models, `gpt-3.5-turbo` and `gpt-4` also accept a so-called system message. These models are specifically trained to follow the role that is explained to them in this system message. The `gpt-3.5-turbo` message does not always pay strong attention to this system message, however, GPT-4 does. \n",
    "\n",
    "The default system message is \"You are a helpful assistant.\"\n",
    "\n",
    "For more details, see: https://platform.openai.com/docs/guides/chat/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19c12b8a-410a-4b61-8b04-e77764877403",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a serious research assistant who follows exact instructions and returns only valid JSON.\"\n",
    "## Note, the system message also counts towards our token usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51e488-1617-4122-8527-14f87214ee6b",
   "metadata": {},
   "source": [
    "### Make a generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b68a59-018f-40c0-83e3-632d577bdbf6",
   "metadata": {},
   "source": [
    "#### The input\n",
    "\n",
    "Let's first generate a prediction for the FLS example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9647bb59-91c5-4648-82f0-2c41530cb1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classify whether the statement below contains a forward looking statements (fls).\n",
      "Rules:\n",
      "- Answer using JSON in the following format: {\"contains_fls\" : 0 or 1}\n",
      "Statement:\n",
      "> We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.\n",
      "JSON =\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template_v2.format(**{\n",
    "    \"statement\" : fls_row[\"statement\"].strip()\n",
    "})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a0c27-1d89-4a94-8e05-441eba1abeb9",
   "metadata": {},
   "source": [
    "#### The request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6964de99-aebc-4e6e-a726-f0ee260da815",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_fls_1 = openai.ChatCompletion.create(\n",
    "    model = model,\n",
    "    temperature = temperature,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\" : system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6215e-7143-47c0-b5d5-daf1dee971cb",
   "metadata": {},
   "source": [
    "### The result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c85fdd20-a110-4962-85e7-1cdc9d8153c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'usage', 'choices'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_fls_1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef498e54-86ce-40cf-8329-13ce796ecba5",
   "metadata": {},
   "source": [
    "**How many tokens did we just use?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6335e205-75b3-47a5-8b95-29aa5be8fd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 97, 'completion_tokens': 9, 'total_tokens': 106}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(completion_fls_1.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1473686-74a4-4c70-a06c-166fead3efea",
   "metadata": {},
   "source": [
    "Our request used up 106 tokens in total, 97 in the prompt and 9 in the completion.\n",
    "\n",
    "This is higher than our estimate through `tiktoken` because the chat models add special tokens to the prompt. \n",
    "\n",
    "We can use the April 2023 pricing to estimate how much our 60 observations will cost to predict:\n",
    "- **gpt-3.5-turbo:** \\\\$0.002 / 1K tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c914fe-abd9-4bf6-9f81-bad37a802594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Running **60** predictions will cost around **~$0.01296**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_tokens = 108\n",
    "num_obs = 60\n",
    "price_per_token = 0.002\n",
    "\n",
    "cost = (num_tokens/1000) * price_per_token * num_obs\n",
    "mprint(f\"\"\"Running **{num_obs}** predictions will cost around **~${cost}**.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8f905-fa85-4e23-a136-e84de8df4496",
   "metadata": {},
   "source": [
    "**Did it work?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cf5d513-f325-4541-860b-35503eb1686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': '{\"contains_fls\" : 1}'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = dict(completion_fls_1[\"choices\"][0][\"message\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7744220-f452-4439-85d6-45fda9c67363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contains_fls': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = json.loads(result[\"content\"])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f908a-2205-4c5c-99cc-4a05067fe2cf",
   "metadata": {},
   "source": [
    "## Let's wrap that up into some functions\n",
    "\n",
    "We can wrap the above logic up into a function so that we can scale thing more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "337ab99f-1aad-4648-b201-0f27459f2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data, prompt_template):\n",
    "    prompt = prompt_template.format(**{\n",
    "        \"statement\" : data[\"statement\"].strip()\n",
    "    })\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2550c504-cc75-4a64-9732-e1a6d48e69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(\n",
    "    i, ## Adding an identifier will make things easier to track and match up later.\n",
    "    prompt, \n",
    "    model = model,\n",
    "    temperature = temperature,\n",
    "    system_message = system_message\n",
    "    ):\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        temperature = temperature,\n",
    "        stop = [\"}\"], ## This forces the model to stop when it hits a closing brace\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\" : system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    result = dict(completion[\"choices\"][0][\"message\"])\n",
    "    prediction = json.loads(result[\"content\"] + \"}\") \n",
    "    ## The end is not included, so we add it back, hence the + \"}\"\n",
    "    prediction[\"i\"] = i\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f73c51-9170-4c5f-a307-f6061f9e5758",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5dd9523-31ee-4ccf-824c-2a7b18987d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classify whether the statement below contains a forward looking statements (fls).\n",
      "Rules:\n",
      "- Answer using JSON in the following format: {\"contains_fls\" : 0 or 1}\n",
      "Statement:\n",
      "> We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.\n",
      "JSON =\n",
      "\n",
      "{'contains_fls': 1, 'i': 2}\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_prompt(fls_row, prompt_template_v2)\n",
    "prediction = make_prediction(fls_row[\"i\"], prompt)\n",
    "print(prompt, end = \"\\n\\n\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f1d0f24-c442-43bd-982b-a637e86f1688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classify whether the statement below contains a forward looking statements (fls).\n",
      "Rules:\n",
      "- Answer using JSON in the following format: {\"contains_fls\" : 0 or 1}\n",
      "Statement:\n",
      "> In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.\n",
      "JSON =\n",
      "\n",
      "{'contains_fls': 0, 'i': 1}\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_prompt(non_fls_row, prompt_template_v2)\n",
    "prediction = make_prediction(non_fls_row[\"i\"], prompt)\n",
    "print(prompt, end = \"\\n\\n\")\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0b4d6-a32b-441c-9f98-5c586d04fa3d",
   "metadata": {},
   "source": [
    "### Let's run it for the full sample of 60 so that we can evaluate\n",
    "\n",
    "Using GLLMs in a zero-shot can create some variation in the prediction behavior, the try/except statement helps to deal with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94163895-13d1-4517-b3c1-e176c559712a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53b9e70ec0846cea518fec29e5696a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_list = []\n",
    "num_tries = 3\n",
    "for item in tqdm(statement_list):\n",
    "    current_try = 0 \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = generate_prompt(item, prompt_template_v2)\n",
    "            prediction = make_prediction(item[\"i\"], prompt)\n",
    "            pred_list.append(prediction)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            current_try += 1\n",
    "            if current_try > num_tries:\n",
    "                print(f\"There is an issue with {item['i']}:\\n{e}\")\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551cb0b-ac21-4401-93a1-b7962cd03110",
   "metadata": {},
   "source": [
    "This took about ~90 seconds to complete for 60 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "306875d5-2668-4be1-9666-1c7570563218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contains_fls</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contains_fls  i\n",
       "0             0  1\n",
       "1             1  2\n",
       "2             0  3\n",
       "3             1  4\n",
       "4             0  5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(pred_list)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942f999-4216-41c4-b5e9-b5c5d0b47094",
   "metadata": {},
   "source": [
    "------\n",
    "# #4 - Evaluate performance\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace6685-c9e9-4e6b-b6ef-e10bc9244704",
   "metadata": {},
   "source": [
    "## Evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78723e-99a3-4dad-b4d0-e40901946362",
   "metadata": {},
   "source": [
    "Our toy example dataset already has truth labels, so we will use that.\n",
    "\n",
    "However, you will generally need to construct a ground-truth evaluation dataset yourself.\n",
    "\n",
    "You can do so by drawing a random sample, storing it without any predictions, and adding the labels yourself. There are tools for this (e.g., `prodigy`), but for most cases Excel will be the easiest tool to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd805215-35f6-486d-9f4c-5cd77606bcba",
   "metadata": {},
   "source": [
    "### Add the predictions to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efe08e8e-5a9d-481f-99df-ff1c2bbce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.rename(columns = {\"contains_fls\" : \"prediction\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0942f576-3341-4757-8270-c5f64ac9234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>statement</th>\n",
       "      <th>contains_fls</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>The successful integration of the acquired com...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Our focus on continuous improvement has enable...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>The resilience of our business model has been ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>Our focus on cost optimization during the last...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Over the next year, we plan to increase our in...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     i                                          statement  contains_fls   \n",
       "20  21  The successful integration of the acquired com...             0  \\\n",
       "34  35  Our focus on continuous improvement has enable...             0   \n",
       "51  52  The resilience of our business model has been ...             0   \n",
       "38  39  Our focus on cost optimization during the last...             0   \n",
       "49  50  Over the next year, we plan to increase our in...             1   \n",
       "\n",
       "    prediction  \n",
       "20           0  \n",
       "34           0  \n",
       "51           0  \n",
       "38           0  \n",
       "49           1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.merge(statement_df, pred_df, on = \"i\", how = \"left\")\n",
    "eval_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae6468-b1da-41fe-8f44-8f0b295e7b5e",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Because we designed our prompt to result a simple 0 and 1, we can treat this like a supervised machine learning approach. For example, we can calculate a classification report using the `scikit-learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57f8f34c-bced-4b78-8076-5752987503d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        30\n",
      "           1       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.98      0.98      0.98        60\n",
      "weighted avg       0.98      0.98      0.98        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    eval_df[\"contains_fls\"], \n",
    "    eval_df[\"prediction\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1dd89-40e8-4843-8825-9826314c0001",
   "metadata": {},
   "source": [
    "Our prediction pipeline works! This is a simple task and ChatGPT is a powerful model, so this is not surprising. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e39ef-7448-4b1b-bab4-3675f0d5a347",
   "metadata": {},
   "source": [
    "----\n",
    "# #5 - Execute\n",
    "---\n",
    "\n",
    "Once we are happy with our approach we can implement the remaining code to run it for all the obervations. \n",
    "\n",
    "A few practical recommendations:\n",
    "\n",
    "1. Always make sure to store all your raw predictions. \n",
    "2. Optimize your prompt to reduce the tokens and costs.    \n",
    "3. Optimize your prediction pipeline to make things faster.  \n",
    "\n",
    "I elaborate on these points below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833192e-6c0c-46b2-b4bf-9584ec258f7c",
   "metadata": {},
   "source": [
    "---\n",
    "### Storing predictions\n",
    "\n",
    "Below is an example of how one can store the raw predictions right after they are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790e01f-fad4-4762-8d8c-e6a61faaa2cb",
   "metadata": {},
   "source": [
    "#### First we create a place to store the data:\n",
    "*Note:* I have a specific folder structure that I like to use to keep things organized. In that stucture I would store these to the \"store\" pipeline folder. You can read more about that here:\n",
    "\n",
    "https://medium.com/towards-data-science/how-to-keep-your-research-projects-organized-part-1-folder-structure-10bd56034d3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21a3a64b-2eb6-4366-8cf9-bea6ec3aef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_loc = Path.cwd() / \"storage\"\n",
    "if not store_loc.exists():\n",
    "    os.makedirs(store_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22547a-a30d-4b00-9037-95de36389ba0",
   "metadata": {},
   "source": [
    "#### Next we adapt the function to store the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e137aff-a491-4c19-8622-102b6c3b01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(\n",
    "    i, \n",
    "    prompt, \n",
    "    model = model,\n",
    "    temperature = temperature,\n",
    "    system_message = system_message,\n",
    "    store_loc = store_loc ## We add the storage location to our function\n",
    "    ):\n",
    "    \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        temperature = temperature,\n",
    "        stop = [\"}\"], \n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\" : system_message},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    result = dict(completion[\"choices\"][0][\"message\"])\n",
    "    prediction = json.loads(result[\"content\"] + \"}\") \n",
    "    prediction[\"i\"] = i\n",
    "    \n",
    "    ## Before returning it, we store the data:\n",
    "    \n",
    "    ### We never want to overwrite our raw predictions, so we add the timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "    with open(store_loc / f\"{i}-=-{timestamp}.json\", \"w\", encoding = \"utf-8\") as f:\n",
    "        json.dump(prediction, f)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7f47c-fd9a-4b93-9d20-4e77342c3a35",
   "metadata": {},
   "source": [
    "#### Now we always store our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "460b27ce-9305-4f6a-98cf-cbd09ce183e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_prompt(fls_row, prompt_template_v2)\n",
    "prediction = make_prediction(fls_row[\"i\"], prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "517731f1-69ce-4098-84ac-236a4cab6077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-=-20230424_184659_929113.json', '2-=-20230425_150752_851815.json']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(store_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9606f9-7b83-4ae9-b66f-cefa3b24b614",
   "metadata": {},
   "source": [
    "----\n",
    "### How much would it cost to scale it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443cc3bd-5e94-4a55-aaac-12597421238b",
   "metadata": {},
   "source": [
    "How much would it cost to classify, let's say, 100,000 sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f265c8c6-5a09-436b-b196-12d9260bfc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Running **100,000** predictions will cost around **~$24.0**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_tokens = 120 ## Rough estimate, slightly higher than what we've seen.\n",
    "num_obs = 100_000\n",
    "price_per_token = 0.002\n",
    "\n",
    "cost = (num_tokens/1000) * price_per_token * num_obs\n",
    "mprint(f\"\"\"Running **{num_obs:,}** predictions will cost around **~${cost}**.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b0c13-dfe7-4d1d-a3ee-b607149ad134",
   "metadata": {},
   "source": [
    "-----\n",
    "## How much time would it take to scale it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b7f54-40a1-4b6f-afe4-26d7782f6c40",
   "metadata": {},
   "source": [
    "#### How much time would it take to run 100,000?\n",
    "\n",
    "It took about 90 seconds to run 60 observations.\n",
    "\n",
    "I.e., about 2 seconds per observation. That is slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db1ba47a-92bf-4032-9bf1-b01c99b63442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Running **100,000** predictions will take around **~55.56** hours."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_obs = 100_000\n",
    "sec_per_obs = 2\n",
    "\n",
    "total_seconds = sec_per_obs * num_obs\n",
    "mprint(f\"\"\"Running **{num_obs:,}** predictions will take around **~{total_seconds/60/60:.2f}** hours.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03c5aa-3adb-422c-9510-2ceb0e4e0ab7",
   "metadata": {},
   "source": [
    "#### How do we make it faster?\n",
    "\n",
    "Generating predictions for 100,000 observations would take about 2.5 days, that is really slow...\n",
    "\n",
    "Here are a few tips and tricks to speed things up:\n",
    "\n",
    "**1. Don't run predictions on data you don't need predictions for.**\n",
    "\n",
    "Is there any filtering or pre-work that we can do to reduce the number of predictions that require a GLLM generation? In this case we could, for example, create a rough word list. If we make the word list over-inclusive and biased toward false positives we can then feed all the potential matches through the GLLM approach to clean it up.\n",
    "\n",
    "**2. We can combine multipe predictions into one.**\n",
    "\n",
    "The speed of the generation scales linearly with the number of tokens, which includes the instructions. Asking the model to make multiple predictions using a single set of instructions will thus be cheaper and faster (although it might not always work).\n",
    "\n",
    "To illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c6754e4-fd37-4f6d-ab05-e30ea5d251de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: classify whether the statements below contain a forward looking statements (fls).\n",
      "Rules:\n",
      "- Answer using JSON in the following format: [{\"contains_fls\" : 0 or 1, \"i\" : i}, ..]\n",
      "Statements:\n",
      "[{'i': 1, 'statement': 'In the last quarter, we managed to increase our revenue by 15% due to the successful launch of our new product line.', 'contains_fls': 0}, {'i': 2, 'statement': 'We anticipate that our investments in R&D will lead to a 20% improvement in efficiency in the next two years.', 'contains_fls': 1}, {'i': 3, 'statement': 'Our recent acquisition of XYZ Company has already started to show positive results in terms of cost savings and market reach.', 'contains_fls': 0}]\n",
      "JSON =\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'contains_fls': 0, 'i': 1},\n",
       " {'contains_fls': 1, 'i': 2},\n",
       " {'contains_fls': 0, 'i': 3}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_multiple = \"\"\"\n",
    "Task: classify whether the statements below contain a forward looking statements (fls).\n",
    "Rules:\n",
    "- Answer using JSON in the following format: [{{\"contains_fls\" : 0 or 1, \"i\" : i}}, ..]\n",
    "Statements:\n",
    "{statements}\n",
    "JSON =\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = prompt_template_multiple.format(**{\"statements\" : str(statement_list[:3])})\n",
    "print(prompt, end = \"\\n\\n\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    temperature = 0,\n",
    "    stop = [\"]\"],\n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    ")\n",
    "\n",
    "result = dict(completion[\"choices\"][0][\"message\"])\n",
    "prediction = json.loads(result[\"content\"] + \"]\") \n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8486d-6559-47c4-a7fc-1a60a4ccfb85",
   "metadata": {},
   "source": [
    "**3. We can saturate the rate limits by making async requests**\n",
    "\n",
    "The rate limits for the ChatGPT API are (as of April 2023):\n",
    "\n",
    "- 3,500 RPM (Requests-per-minute)\n",
    "- 90,000 TPM (Tokens-per-minute)\n",
    "\n",
    "**Theoretical throughput:** 90,000 / 120 tokens = 750 predictions per minute    \n",
    "Versus   \n",
    "**Sequential throughput:** 30 predictions per minute\n",
    "\n",
    "So we can potentially speed things up 12x with an async approach!\n",
    "\n",
    "For an implementation example, see the OpenAI Cookbook:\n",
    "\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/297c53430cad2d05ba763ab9dca64309cb5091e9/examples/api_request_parallel_processor.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
